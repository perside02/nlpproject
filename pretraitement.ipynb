{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ad29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer les biblioth√®ques n√©cessaire\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb824aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126f1af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af86892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©but du pr√©traitement des donn√©es...\n",
      "\n",
      "Fusion termin√©e - Dimensions du DataFrame fusionn√©: (172, 9)\n",
      "\n",
      "Colonnes de texte identifi√©es: [\"Qu'est-ce que vous appr√©ciez le plus √† l'UV-BF ?\", \"Quelles sont les principales difficult√©s ou aspects n√©gatifs que vous rencontrez √† l'UV-BF ?\", 'profile_name', 'post_text', 'comments']\n",
      "Traitement de la colonne: Qu'est-ce que vous appr√©ciez le plus √† l'UV-BF ?\n",
      "  Exemple 1:\n",
      "    Original: Les cours sont int√©ressants mais parfois difficiles √† suivre √† cause des coupure...\n",
      "    Pr√©trait√©: ['cour', 'int√©ress', 'parfois', 'difficil', 'suivr', 'caus', 'coupur', 'connexion']\n",
      "  Exemple 2:\n",
      "    Original: Oui je pense que il ya des failles ! La connexion n'est pas stable, et en plus l...\n",
      "    Pr√©trait√©: ['oui', 'pens', 'faill', 'connexion', 'nest', 'stabl', 'plus', 'qualit', 'cour', 'nest', 'trop']\n",
      "Traitement de la colonne: Quelles sont les principales difficult√©s ou aspects n√©gatifs que vous rencontrez √† l'UV-BF ?\n",
      "  Exemple 1:\n",
      "    Original: Les √©tudes √† l'Uv, c'est vraiment pas facile avec les probl√®mes de connexion r√©c...\n",
      "    Pr√©trait√©: ['√©tud', 'luv', 'cest', 'vrai', 'facil', 'problem', 'connexion', 'r√©curent']\n",
      "  Exemple 2:\n",
      "    Original: Probl√®me li√© √† la connexion ...\n",
      "    Pr√©trait√©: ['problem', 'li', 'connexion']\n",
      "Traitement de la colonne: profile_name\n",
      "  Exemple 1:\n",
      "    Original: uvburkina...\n",
      "    Pr√©trait√©: ['uvburkin']\n",
      "  Exemple 2:\n",
      "    Original: uvburkina...\n",
      "    Pr√©trait√©: ['uvburkin']\n",
      "Traitement de la colonne: post_text\n",
      "  Exemple 1:\n",
      "    Original: ùêîùêï-ùêÅùêÖ : ùêãùêöùêßùêúùêûùê¶ùêûùêßùê≠ ùêùùêû ùê•ùêö ùêùùêûùêÆùê±ùê¢ùêûÃÄùê¶ùêû ùê¨ùêûùê¨ùê¨ùê¢ùê®ùêß ùêù‚ÄôùêûÃÅùêØùêöùê•ùêÆùêöùê≠ùê¢ùê®ùêß ùêùùêûùê¨ ùê´ùêûùê¨ùê¨ùê®ùêÆùê´ùêúùêûùê¨ ùê©ùêûÃÅùêùùêöùê†ùê®ùê†ùê¢...\n",
      "    Pr√©trait√©: ['lunivers', 'virtuel', 'burkin', 'faso', 'uvbf', 'proc√©d√©en', 'voir', 'plus']\n",
      "  Exemple 2:\n",
      "    Original: ùêàùêßùêØùê¢ùê≠ùêöùê≠ùê¢ùê®ùêß ùêöùêÆùê± ùêßùê®ùêÆùêØùêûùêöùêÆùê± ùêõùêöùêúùê°ùêûùê•ùê¢ùêûùê´ùê¨Le Minist√®re de l‚ÄôEnseignement‚Ä¶En voir plus...\n",
      "    Pr√©trait√©: ['minister', 'lenseignementen', 'voir', 'plus']\n",
      "Traitement de la colonne: comments\n",
      "  Exemple 1:\n",
      "    Original: Universit√© Virtuelle du Burkina Faso - UVBFbonjour vous pouvez songer √† r√©pondre...\n",
      "    Pr√©trait√©: ['univers', 'virtuel', 'burkin', 'faso', 'uvbfbonjour', 'pouv', 'song', 'r√©pondr', 'question', 'gen', 'laiss', 'num√©ro', 'renseign', 'merc', 'beaucoup', 'pos', 'question', 'inbox', 'depuuiis']\n",
      "  Exemple 2:\n",
      "    Original: Attendez s√©rieusementUniversit√© Virtuelle du Burkina Faso - UVBFheein , Nos Ordi...\n",
      "    Pr√©trait√©: ['attend', 's√©rieusementunivers', 'virtuel', 'burkin', 'faso', 'uvbfheein', 'ordin', 'vont', 'ven', 'quand', 'attend', 'depuis', 'plus', 'deux', 'mois', 'travaill', 'betsal√©el', 'nacoulmafranch', 'cest', 'int√©ress', 'postul', 'depuis', 'mai']\n",
      "Nombre total d'observations: 172\n",
      "Nombre total de tokens dans Qu'est-ce que vous appr√©ciez le plus √† l'UV-BF ?: 1469\n",
      "Nombre total de tokens dans Quelles sont les principales difficult√©s ou aspects n√©gatifs que vous rencontrez √† l'UV-BF ?: 3045\n",
      "Nombre total de tokens dans profile_name: 10\n",
      "Nombre total de tokens dans post_text: 61\n",
      "Nombre total de tokens dans comments: 119\n",
      "Pr√©traitement terminer\n",
      "Donn√©es trait√©es : 172 lignes \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def nettoyer_texte(texte):\n",
    "    \"\"\"\n",
    "    Nettoie le texte en supprimant mentions, hashtags, URLs, ponctuation, emojis\n",
    "    \"\"\"\n",
    "    if not isinstance(texte, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Supprimer les mentions @\n",
    "    texte = re.sub(r\"@\\w+\", \"\", texte)\n",
    "    # Supprimer les hashtags #\n",
    "    texte = re.sub(r\"#\\w+\", \"\", texte)\n",
    "    # Supprimer les URLs\n",
    "    texte = re.sub(r\"http\\S+|www\\.\\S+\", \"\", texte)\n",
    "    # Supprimer la ponctuation et caract√®res sp√©ciaux (garder lettres et accents)\n",
    "    texte = re.sub(r\"[^a-z√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ß\\s]\", \"\", texte, flags=re.IGNORECASE)\n",
    "    # Convertir en minuscules\n",
    "    texte = texte.lower()\n",
    "    # Supprimer les espaces multiples\n",
    "    texte = re.sub(r\"\\s+\", \" \", texte)\n",
    "    \n",
    "    return texte.strip()\n",
    "\n",
    "# Initialiser les ressources fran√ßaises\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "\n",
    "def preprocess_text(texte):\n",
    "    \"\"\"\n",
    "    Pr√©traitement complet du texte\n",
    "    \"\"\"\n",
    "    # Nettoyage de base\n",
    "    texte_nettoye = nettoyer_texte(texte)\n",
    "    \n",
    "    # Tokenisation\n",
    "    tokens = word_tokenize(texte_nettoye, language='french')\n",
    "    \n",
    "    # Suppression des stopwords et stemming\n",
    "    tokens_filtres = [\n",
    "        stemmer.stem(token) \n",
    "        for token in tokens \n",
    "        if token not in french_stopwords and len(token) > 2\n",
    "    ]\n",
    "    \n",
    "    return tokens_filtres\n",
    "\n",
    "def fusionner_et_pretraiter():\n",
    "    # 1. Charger les deux fichiers\n",
    "    df_uvbf = pd.read_csv('data_uvbf.csv', encoding='utf-8')\n",
    "    df_facebook = pd.read_csv('facebook_posts.csv', encoding='utf-8')\n",
    "    \n",
    "    # 2. Ajouter une colonne pour identifier la source\n",
    "    df_uvbf['source'] = 'sondage'\n",
    "    df_facebook['source'] = 'facebook'\n",
    "    \n",
    "    # 3. Fusionner les deux dataframes\n",
    "    df_fusion = pd.concat([df_uvbf, df_facebook], ignore_index=True, sort=False)\n",
    "    \n",
    "    print(\"\\nFusion termin√©e - Dimensions du DataFrame fusionn√©:\", df_fusion.shape)\n",
    "    \n",
    "    # 4. Identifier toutes les colonnes de texte\n",
    "    colonnes_texte = []\n",
    "    for colonne in df_fusion.columns:\n",
    "        if df_fusion[colonne].dtype == 'object' and colonne not in ['source', 'Timestamp', 'profile_url', 'extracted_at']:\n",
    "            colonnes_texte.append(colonne)\n",
    "\n",
    "    \n",
    "    print(f\"\\nColonnes de texte identifi√©es: {colonnes_texte}\")\n",
    "    \n",
    "    # 5. Appliquer le pr√©traitement sur chaque colonne de texte\n",
    "    for colonne in colonnes_texte:\n",
    "        print(f\"Traitement de la colonne: {colonne}\")\n",
    "        \n",
    "        # Cr√©er une nouvelle colonne pour les tokens pr√©trait√©s\n",
    "        nouvelle_colonne = f\"{colonne}_pretraite\"\n",
    "        df_fusion[nouvelle_colonne] = df_fusion[colonne].apply(preprocess_text)\n",
    "        \n",
    "        # Afficher un √©chantillon des r√©sultats\n",
    "        samples = df_fusion[df_fusion[colonne].notna()].head(2)\n",
    "        for i, (orig, pretrait) in enumerate(zip(samples[colonne], samples[nouvelle_colonne])):\n",
    "            if pd.notna(orig):\n",
    "                print(f\"  Exemple {i+1}:\")\n",
    "                print(f\"    Original: {str(orig)[:80]}...\")\n",
    "                print(f\"    Pr√©trait√©: {pretrait}\")\n",
    "\n",
    "    df_fusion['texte_complet_pretraite'] = df_fusion[[f\"{col}_pretraite\" for col in colonnes_texte]].apply(\n",
    "        lambda row: [token for col_pretraite in row.index for token in row[col_pretraite] if isinstance(row[col_pretraite], list)], \n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 7. Statistiques sur le pr√©traitement\n",
    "    print(f\"Nombre total d'observations: {len(df_fusion)}\")\n",
    "\n",
    "    for colonne in colonnes_texte:\n",
    "        nouvelle_colonne = f\"{colonne}_pretraite\"\n",
    "        nb_tokens = df_fusion[nouvelle_colonne].apply(lambda x: len(x) if isinstance(x, list) else 0).sum()\n",
    "        print(f\"Nombre total de tokens dans {colonne}: {nb_tokens}\")\n",
    "           \n",
    "    \n",
    "    # 6. Sauvegarder le fichier pretraiter\n",
    "    df_fusion.to_csv('donnees_pretraitees.csv', index=False, encoding='utf-8')\n",
    "    return df_fusion\n",
    "\n",
    "# Ex√©cuter la fonction\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"D√©but du pr√©traitement des donn√©es...\")\n",
    "    df_resultat = fusionner_et_pretraiter()\n",
    "    print(\"Pr√©traitement terminer\")\n",
    "    print(f\"Donn√©es trait√©es : {len(df_resultat)} lignes \")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
